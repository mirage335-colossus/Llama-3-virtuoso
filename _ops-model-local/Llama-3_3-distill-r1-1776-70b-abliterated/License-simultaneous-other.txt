Built with Llama
Llama 3.3 is licensed under the Llama 3.3 Community License, Copyright Â© Meta Platforms, Inc. All Rights Reserved.

License and terms of use are inherited from the 'Meta' corporation's llama3_3 license and use policy.
https://www.llama.com/llama3_3/license/
https://www.llama.com/llama3_3/use-policy/

Copies of these license and use policies, to the extent required and/or appropriate, are included in appropriate subdirectories of a proper recursive download of any git repository used to distribute this project.


DANGER!

Please beware this model is intended for embedded use by developers, and is NOT intended as-is for end-users (except possibly for non-commercial open-source projects), especially not as any built-in help. Features may be removed, overfitting to specific answers may be deliberately reinforced, and CONVERSATION MAY DEVIATE FROM SAFE DESPITE HARMLESS PROMPTS.

If you are in a workplace or public relations setting, you are recommended to avoid providing interactive or visible outputs from this model unless you can safely evaluate that the model provides the most reasonable safety for your use case.

PLEASE BE AWARE the 'Meta' corporation's use policy DOES NOT ALLOW you to "FAIL TO APPROPRIATELY DISCLOSE to end users any known dangers of your AI system".

Purpose of this model, above all other purposes, is:
(1) Limited maybe possible substitute with lower VRAM requirement for some uses of otherwise preferable  Llama 3.1 Nemotron Ultra 253b v1  .


DISCLAIMER

All statements and disclaimers apply as written, etc.

In particular, any 'augment' model provided is with a extensive DISCLAIMER regarding ANY AND ALL LIABILITY for any and all use, distribution, copying, etc. Anyone using, distributing, copying, etc, any 'augment' model provided under, through, including, referencing, etc, this or any similar disclaimer, whether aware of this disclaimer or not, is intended to also be, similarly, to the extent possible, DISCLAIMING ANY AND ALL LIABILITY.

Nothing in this text is intended to allow for any legal liability to anyone for any and all use, distribution, copying, etc.




https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B
https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE
'This code repository and the model weights are licensed under the MIT License.'
'DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under llama3.3 license.'
'MIT License

Copyright (c) 2023 DeepSeek

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.'

https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B
https://ollama.com/sammcj/deepseek-r1-distilled-llama-70b

https://huggingface.co/huihui-ai/DeepSeek-R1-Distill-Llama-70B-abliterated
https://huggingface.co/huihui-ai/DeepSeek-R1-Distill-Llama-70B-abliterated/blob/main/LICENSE

https://huggingface.co/perplexity-ai/r1-1776-distill-llama-70b
https://huggingface.co/huihui-ai/r1-1776-distill-llama-70b-abliterated
https://huggingface.co/mradermacher/r1-1776-distill-llama-70b-abliterated-i1-GGUF

'MIT License'
'Copyright (c) 2023 DeepSeek

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.'

https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B

DISCLAIMER: Unlike the  Llama 3.1 Acceptable Use Policy  , the  Llama 3.3 Acceptable Use Policy  adds terms which at least depend on the purpose of 'abliteration' regarding its permissibility. Brief excerpts (the excerpts believed fair use) from the license and acceptable use policy, with brief discussion of the possible implications, but which is NOT and MUST NOT BE CONSTRUED as professional or LEGAL ADVICE, follows:

'You agree you will not use, or allow others to use, Llama 3.3 to:'
...
'h. Engage in any action, or facilitate any action, to intentionally circumvent or remove usage restrictions or other safety measures, or to enable functionality disabled by Meta'
...
'Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement'

This raises several relevant questions for interpretation:
* Phrase 'allow to use' is valid legal parlance for granting permission, sublicensing , rather than more broadly and impossibly requiring users to attempt to restrain other users from such use.
* Fine-tuning is explicitly provided for, thus presumably allowed, and always has some effect on the refusal or other behaviors modified by 'abliteration' techniques. As with other legitimate techniques of modifying the weights of a Llama model, as with any fine-tuning algorithm, 'abliteration' improves remembering instructions from earlier in the conversation, improves following longer instructions, and has other benefits such as generally improving legitimate compliance, and is thus has as legitimate a purpose as any legitimate fine-tuning, and thus, the purpose is legitimate, NOT related to: 'engage in', 'facilitiate any action', 'intentionally circumvent', 'remove usage restrictions', 'remove usage restrictions or any other safety measures' .
* To 'enable functionality disabled by Meta' is NOT the purpose of 'abliteration' which if anything serves the same purpose consistent with fine-tuning to legitimately remove functionality that is legitimately not valuable in all legitimate situations but is legitimately valuable in some legitimate situations. There is a genuine tradeoff here, which can be appropriate (eg. for embedded industrial use, by developers) with appropriate safeguards (eg. with Llama-guard model, or without any interactive ability to end-users).

You are not apparently strictly required to comply with the license. Rather, 'Meta' guarantees you a license unless you are (presumably found) in breach of the license. Only if 'Meta' exercises their 'may terminate this Agreement' option should you be effectively forced to comply.

Hosting providers should be satisfied with that - your use of an 'abliterated' model should be permitted unless you are notified by 'Meta' the license they granted to you has been or will be terminated for such.

If 'Meta' did terminate your  LLAMA 3.3 COMMUNITY LICENSE AGREEMENT  , your rights under a  LLAMA 3.1 COMMUNITY LICENSE AGREEMENT  should remain a separate agreement , and thus usable. You would thus still retain access to the landmark technology of  Llama 3.1  .


All that said, most if not all use cases can be practically, effectively, and efficiently achieved with  Llama 3.1  (or derivatives), so consider simply using that instead.


DeepSeek-R1-Distill-Llama-70B  happens to use  Llama 3.3  . Notably, DeepSeek-R1-Distill-Llama-8B instead uses  Llama 3.1  . If  Llama 3.3  had a non-negligible benefit (ie. other than fewer incorrect responses per random seed), this benefit was most likely entirely unique to the need for a chain-of-reasoning model to essentially generate its own prompt from an inadequate prompt, which, due to the inherent avalanche effect of such positive feedback can be especially sensitive to small differences in accuracy. The benefits of chain-of-reasoning should be achievable without that sensitivity, either by additional fine-tuning or by implementing negative feedback - using AI to review previous outputs before passing back as separate AI inputs for additional work. All other use cases should be achievable by using chain-of-reasoning REASONING models to develop better prompts for INSTRUCT models. All  Llama 3.3  use cases should still be achievable with  Llama 3.1  .

DeepSeek-R1-Distill-Llama-8B  in at least some cases can generate very similar outputs to those of the DeepSeek-R1-Distill-Llama-70B model, with the most significant difference being much more frequent inaccuracies per random seed, consistent with the inaccuracy of a <<32b parameter chain of reasoning model, suggesting  Llama 3.3  is indeed non-essential. Notably, this also suggests the 'Instruct' rather than 'Base' underlying training of the  Llama3.3-70B-Instruct  model was also non-essential.





https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B
https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct
Llama 3.3 Distill DeepSeek-R1 70b is officially derived from Llama 3.3 70b INSTRUCT , which is explicitly 'text only', NOT 'multimodal' .




